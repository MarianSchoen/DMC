---
title: "report"
output: pdf_document
params:
   tempdir: ""
   metric: "cor"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
temp.dir <- params$tempdir
benchmark.name <- strsplit(temp.dir, "/")[[1]][length(strsplit(temp.dir, "/")[[1]])]
# read input and processed data
input_raw <- read_data(paste(temp.dir, "/input_data/raw.h5", sep=""))
sc.counts <- input_raw$sc.counts
sc.pheno <- input_raw$sc.pheno
real.counts <- input_raw$bulk.counts
real.props <- input_raw$bulk.props
validation_set <- read_data(paste(temp.dir, "/input_data/validation_set.h5", sep = ""))
training_set <- read_data(paste(temp.dir, "/input_data/training_set.h5", sep=""))
training.exprs <- training_set$sc.counts
training.pheno <- training_set$sc.pheno
test.exprs <- validation_set$sc.counts
test.pheno <- validation_set$sc.pheno
sim.bulks <- list(bulks = validation_set$real.counts, props = validation_set$real.props)
# load input parameters
input_params <- read_misc_input(paste(temp.dir, "input_data/params.h5", sep = "/"))
genesets <- input_params$genesets
algorithm.names <- input_params$algorithms
function.call <- input_params$function.call
grouping <- input_params$grouping
```

# Deconvolution Benchmark

## Input data
Parameters of the `benchmark` call:
```{r}
function.call.list <- as.list(function.call)

adjusted.names <- sapply(
  X = names(function.call.list)
  , FUN = function(x){if(x != ""){return(paste0(x, " ="))}else{return(x)}}
  , USE.NAMES = FALSE
  )
print(paste(adjusted.names, function.call.list)) 
``` 

Single-cell data contained `r nrow(sc.counts)` features for `r ncol(sc.counts)` cells.  


Corresponding phenotype data contained labels for `r length(unique(sc.pheno$cell_type))` cell types: 
```{r}
  t <- table(sc.pheno$cell_type)
  print(paste(names(t), t, sep = ": "))
```


Used algorithms: `r algorithm.names`   


`r ncol(real.counts)` real bulk profiles were available, with `r length(intersect(rownames(real.counts), rownames(sc.counts)))` overlapping features

Ground-truth cell type quantities were available for `r nrow(real.props)` cell types.  


```{r real_props, echo = FALSE}
  print(t(apply(real.props, 1, summary)))
```
\pagebreak
## Deconvolution of real bulks  

```{r real_results, echo=FALSE}
  files <- list.files(paste(temp.dir, "/results/real", sep = ""), full.names = T, pattern = "*.h5")
  results.lists <- list()
  for(i in 1:length(files)) {
    results.lists[[i]] <- read_result_list(files[i])
  }
  
  for(i in 1:length(results.lists)){
  	temp.df <- prepare_data(results.lists[[i]], params$metric)
  	if(!is.null(temp.df)){
  		if(!exists("real.df")){
  			real.df <- temp.df
  		}else{
  			real.df <- rbind(real.df, temp.df)
   		}
  	}
  }
  
  score.plot.list <- evaluation_plot(real.df, "deconvolution quality",params$metric)
  score.plot <- score.plot.list$plot
  celltypes.ordered <- score.plot.list$celltype.order
  algorithms.ordered <- score.plot.list$algorithm.order
  
  runtime.plot <- plot_runtime(real.df, algorithm.order = algorithms.ordered)
  scatter.plots.list <- list()
  for(i in 1:length(results.lists)) {
  	scatter.plots.list[[i]] <- create_scatterplots(results.lists[[i]], celltype.order = celltypes.ordered, algorithm.order = algorithms.ordered)
  }
  cond.num.plots <- plot_cond_num(real.df, metric = params$metric, algorithm.order = algorithms.ordered)
  score.width <- 2 * length(unique(real.df$cell_type))
  score.height <- 2 * length(algorithm.names)
```

### Score plot
```{r score_plot, echo = FALSE, fig.width = score.width, fig.height = score.height}
plot(score.plot)
```  

Deconvolution result for real bulks per algorithm and cell type. The value is the pearson correlation coefficient, coded by rectangle size and color (small to big, red to green with increasing performance). Negative and undefined correlations were set to 0. The overall column contains the average performance of the algorithm across all cell types.
In case of multiple repetitions, the plotted scores are averages over all repetitions. The pearson r specified for each algorithm corresponds to the 'overall' column. Uncertainty (standard deviation) of the overall performance is specified for each algorithm as well.
\pagebreak  

### Runtime plot
```{r runtime_plot, echo = FALSE, fig.width = 16, fig.height = 9}
plot(runtime.plot)
```  
  
Runtime per algorithm. Due to possible large differences in runtime the scale on the y-axis is transformed using the log10. In case of multiple repetitions, the plotted runtimes are averages over all repetitions.
\pagebreak  

### Scatter plots of quantities
```{r scatter_plots, echo = FALSE, fig.width = 18, fig.height = 5}
for(l in scatter.plots.list){
	for(p in l) {
		plot(p)
	}
}
```  

Comparison of the real cell type quantities and the estimated quantities per cell type for each algorithm. Only the results of the first repetition are shown.

\pagebreak
### Condition number plots
```{r condition_plots, fig.width = 16, fig.height = 9}
plot(cond.num.plots$cond_num_plot)
plot(cond.num.plots$cond_vs_score)
plot(cond.num.plots$variation_plot)
```  

1) Average condition numbers of the reference matrices produced by different algorithms (models).  
  
2) Dependency of score (average overall pearson correlation) on condition number of the model  
  
3) Standard deviation of the score (correlation) of the model across multiple repetitions depending on the variability of the model's condition number  
  
\pagebreak

## Simulations
```{r simulation_results, echo = FALSE}
# bulk benchmark
if(dir.exists(paste(temp.dir, "/results/simulation/bulks", sep = ""))) {
	results.lists <- list()
	files <- list.files(paste(temp.dir,"/results/simulation/bulks", sep=""), full.names = T, pattern = "*.h5")
	for(i in 1:length(files)){
		results.lists[[i]] <- read_result_list(files[i])
	}
	bulks.df <- data.frame()
	for(i in 1:length(results.lists)){
		bulks.df <- rbind(bulks.df, prepare_data(results.lists[[i]], params$metric))
	}

	score.plot.sim.list <- evaluation_plot(bulks.df, "deconvolution quality (simulated bulks)", params$metric)
	score.plot.sim <- score.plot.sim.list$plot
	algorithms.ordered <- score.plot.sim.list$algorithm.order
	celltypes.ordered <- score.plot.sim.list$celltype.order
	
	runtime.plot.sim <- plot_runtime(bulks.df, algorithm.order = algorithms.ordered)
	scatter.plots.list.sim <- list()
	for(i in 1:length(results.lists)){
		scatter.plots.list.sim[[i]] <- create_scatterplots(results.lists[[i]], celltype.order = celltypes.ordered, algorithm.order = algorithms.ordered)
	}
	score.width <- 2*length(unique(bulks.df$cell_type))
}else{
  algorithms.ordered = NULL
  celltypes.ordered = NULL
}
# sample benchmark
if(dir.exists(paste(temp.dir, "/results/simulation/samples", sep = ""))){
	results.lists <- list()
	files <- list.files(paste(temp.dir, "/results/simulation/samples", sep = ""), full.names = T, pattern = "*.h5")
	for(i in 1:length(files)){
		results.lists[[i]] <- read_result_list(files[i])
	}
	samples.df <- data.frame()
	for(i in 1:length(results.lists)){
		samples.df <- rbind(samples.df, prepare_data(results.lists[[i]], params$metric))
	}
	sample.plots <- create_boxplots(samples.df, metric = params$metric, celltype.order = celltypes.ordered, algorithm.order = algorithms.ordered)
}

# gene benchmark
if(dir.exists(paste(temp.dir, "/results/simulation/genes", sep=""))){
	results.lists <- list()
	files <- list.files(paste(temp.dir, "/results/simulation/genes/", sep=""), full.names = T, pattern = "*.h5")
	for(i in 1:length(files)){
		results.lists[[i]] <- read_result_list(files[i])
	}
	genes.df <- data.frame()
	for(i in 1:length(results.lists)){
		genes.df <- rbind(genes.df, prepare_data(results.lists[[i]], params$metric))
	}
	gene.plots <- create_lineplots(genes.df, metric = params$metric, genesets, rownames(training.exprs), algorithm.order = algorithms.ordered, celltype.order = celltypes.ordered)
}

# subtype benchmark
if(dir.exists(paste(temp.dir, "/results/simulation/subtypes", sep=""))){
	results.lists <- list()
	files <- list.files(paste(temp.dir, "/results/simulation/subtypes/", sep=""), full.names = T, pattern = "*.h5")
	for(i in 1:length(files)){
		results.lists[[i]] <- read_result_list(files[i])
	}
	subtypes.df <- data.frame()
	for(i in 1:length(results.lists)){
		subtypes.df <- rbind(subtypes.df, prepare_data(results.lists[[i]], params$metric))
	}
	score.plot.subtype <- evaluation_plot(subtypes.df, "deconvolution quality (simulated subtypes)", params$metric, celltype.order = celltypes.ordered, algorithm.order = algorithms.ordered)$plot
	runtime.plot.subtype <- plot_runtime(subtypes.df, algorithm.order = algorithms.ordered)
}

```

### Bulk simulations
### Score plot
```{r bulk_sim_scores, fig.width = score.width, fig.height = score.height}
if(exists("score.plot.sim"))
	plot(score.plot.sim)
```  
  
Deconvolution result for simulated bulks per algorithm and cell type. The value is the pearson correlation coefficient, coded by rectangle size and color (small to big, red to green with increasing performance). Negative and undefined correlations were set to 0. The overall column contains the average performance of the algorithm across all cell types.
In case of multiple repetitions, the plotted scores are averages over all repetitions. The pearson r specified for each algorithm corresponds to the 'overall' column. Uncertainty (standard deviation) of the overall performance is specified for each algorithm as well.  
Bulks were simulated by adding up a fixed number of randomly sampled single-cell profiles.
\pagebreak

### Scatter plot of quantities
```{r bulk_sim_scatter, fig.width = 18, fig.height = 5}
if(exists("scatter.plots.list.sim")){
	for(l in scatter.plots.list.sim){
		for(p in l){
			plot(p)
		}
	}
}
```  

Comparison of the real cell type quantities (in the simulated bulks) and the estimated quantities per cell type for each algorithm. Only the results of the first repetition are shown.

\pagebreak
### Training set size simulation
### Score boxplots
```{r sample_sim, fig.width = 16, fig.height = 9}
if(exists("sample.plots")){
	for(p in sample.plots$cell.type.plots) plot(p)
}
```  

Variability of performance of algorithms using different amounts and selection of training data for learning and reference profile creation. The score is the average correlation across multiple repetitions (if number of repetitions greater than 1). Overall denotes the average correlation coefficient of all cell types. Bulks used for deconvolution were simulated by adding up a fixed number of randomly sampled expression profiles.

\pagebreak
### Geneset simulation

### Runtime lineplot
```{r geneset_sim_time, fig.width = 16, fig.height = 9}
if(exists("gene.plots")){
	plot(gene.plots$runtime.plot)
}
```  
  
Runtime of algorithms depending on the gene set supplied for deconvolution. Each algorithm performed a separate feature selection on the gene set to obtain the features used in the model. Gene sets are ordered by the size of the intersect of features available in the gene set and the single-cell data.

### Score lineplots
```{r geneset_sim, fig.width = 16, fig.height = 9}
if(exists("gene.plots")){
	for(p in gene.plots$cell.type.plots)
		plot(p)
}
```  

Performance of algorithms depending ont the gene set supplied for deconvolution. Each algorithm performed a separate feature selection on the gene set to obtain the features used in the model. Gene sets are ordered by the size of the intersect of features available in the gene set and the single-cell data. Score is the pearson correlation averaged of multiple repetitions. Errorbars show standard deviation of the score across repetitions.

\pagebreak

### Subtype simulation
### Score plot
```{r subtype_sim, fig.width = score.width, fig.height = score.height}
if(exists("score.plot.subtype")){
  plot(score.plot.subtype)
}
```  
  
Deconvolution result for simulated bulks per algorithm and cell type. The value is the pearson correlation coefficient, coded by rectangle size and color (small to big, red to green with increasing performance). Negative and undefined correlations were set to 0. The overall column contains the average performance of the algorithm across all cell types.
In case of multiple repetitions, the plotted scores are averages over all repetitions. The pearson r specified for each algorithm corresponds to the 'overall' column. Uncertainty (standard deviation) of the overall performance is specified for each algorithm as well.  
Bulks were simulated by adding up a fixed number of randomly sampled single-cell profiles.  
For each cell type, a fixed number of subtypes was simulated based on t-SNE embedding and unsupervised clustering. Deconvoution was performed with a reference profile for each subtype and the estimated quantities of all subtypes of the same cell type were added to arrive at the final estimation for each cell type.
\pagebreak
### Runtime plot
```{r, fig.width = 16, fig.height = 9}
if(exists("runtime.plot.subtype")){
  plot(runtime.plot.subtype)
}
```  
  
Runtime for the benchmark described above. Do the runtimes depend on the number of cell types to be estimated?  

